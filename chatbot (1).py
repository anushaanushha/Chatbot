# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XmMAhTJOunzuo3L1P11tMbtlzU7Rv6R9
"""

!pip install streamlit
!pip install PyPDF2
!pip install openai
!pip install langchain
!pip install streamlit-chat

!pip install streamlit PyPDF2 transformers sentence-transformers faiss-cpu pyngrok
!pip install pyngrok

!pip install transformers torch

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from PyPDF2 import PdfReader
# from sentence_transformers import SentenceTransformer
# from transformers import pipeline
# import faiss
# 
# # App Styling
# st.set_page_config(
#     page_title="PDF Chatbot",
#     page_icon="ðŸ¤–",
#     layout="wide",
# )
# 
# st.title("ðŸ¤– Chatbot with PDF")
# st.sidebar.title("Upload a PDF")
# st.sidebar.write("Interact with your PDF by asking questions!")
# 
# # Sidebar for PDF Upload
# uploaded_pdf = st.sidebar.file_uploader("Choose a PDF file", type=["pdf"])
# 
# if uploaded_pdf is not None:
#     # Extract Text from PDF
#     reader = PdfReader(uploaded_pdf)
#     text = ""
#     for page in reader.pages:
#         text += page.extract_text()
#     st.sidebar.success("PDF uploaded and processed successfully!")
# 
#     # Split Text into Chunks
#     chunks = [text[i:i+500] for i in range(0, len(text), 500)]
# 
#     # Build Embeddings and FAISS Index
#     st.write("Building embeddings...")
#     model = SentenceTransformer("all-MiniLM-L6-v2")
#     embeddings = model.encode(chunks)
#     d = embeddings.shape[1]
#     index = faiss.IndexFlatL2(d)
#     index.add(embeddings)
# 
#     st.success("Embeddings created and indexed!")
# 
#     # Load Chatbot Model
#     chatbot_model = pipeline("text2text-generation", model="google/flan-t5-small")
# 
#     # Chat Interface
#     st.subheader("Ask your question:")
#     user_query = st.text_input("Type your question here:")
#     if st.button("Ask"):
#         if user_query:
#             query_embedding = model.encode([user_query])
#             _, indices = index.search(query_embedding, k=3)
#             relevant_chunks = [chunks[i] for i in indices[0]]
#             context = " ".join(relevant_chunks)
#             prompt = f"Context: {context}\n\nQuestion: {user_query}\n\nAnswer:"
#             response = chatbot_model(prompt, max_length=100, num_return_sequences=1)[0]["generated_text"]
#             st.markdown(f"*Response:* {response}")
#         else:
#             st.warning("Please enter a question!")
# else:
#     st.write("Upload a PDF file to get started!")

!ngrok kill

!streamlit run app.py --server.port 8501 &>/content/logs.txt &
# Authenticate ngrok
!ngrok authtoken <token> # replace <your-authtoken> with your ngrok authtoken

from pyngrok import ngrok
public_url = ngrok.connect(8501)
print(f"Access your app here: {public_url}")